# -*- coding: utf-8 -*-
# This code takes as an argument the files generated by TransformArraySparseFloat.py and outputs their projections
# using the appropriate quartile file. The output directory needs to be changed manually if needed.
# Example usage:
# python opu_mapping.py --type ttbar --ncomp 50000
#TODO: make new dir in opuouts rather than just files to make analysis script easier to use

import argparse

import numpy as np
import pandas as pd

import sparse
from scipy.ndimage.filters import maximum_filter
from tqdm import tqdm

from lightonml.projections.sklearn import OPUMap


if __name__ == '__main__':
    parser = argparse.ArgumentParser("OPU transform images")
    parser.add_argument('--type', type=str, default='ttbar', help="Collision type, ttbar or W")
    parser.add_argument('--ncomp', type=int, default=30000, help="Number of random features desired")
    args = parser.parse_args()

    collision_type = args.type
    n_comp = args.ncomp

    print(f'Using {collision_type} filter.')
    quartile_filter = pd.read_csv("../quartile_filters_" + collision_type + ".csv",
                                  index_col=0)

    filenames = ['ECAL_ForwardN', 'ECAL_EBEE', 'ECAL_ForwardP',
                 'ECAL_Gamma_ForwardN', 'ECAL_Gamma_EBEE', 'ECAL_Gamma_ForwardP',
                 'HCAL_ForwardN', 'HCAL_HEN', 'HCAL_HB', 'HCAL_HEP', 'HCAL_ForwardP']

    namedir = "../ArrayInputs_900/fmixed_data_"
    print(f'OPU mapping with {n_comp} components.')
    random_mapping = OPUMap(n_components=n_comp,
                            ndims=2)


    def file2arr(filestub):
        '''
        Transform the files of the 'filestub'-th generated process into a
        set of canvas and labels.

        Parameters
        ----------
        filestub: int,
            the number characterizing the process considered. Must be padded
            with zeros to be of length 3

        Returns
        ----------
        canvas: np.ndarray,
            boolean array to be mapped to the DMD
        labels: np.ndarray
        '''
        f = {}

        labels = np.load(namedir + filestub + ".h5Labels.npz")['data']
        nevents = labels.shape[0]
        for i, fn in enumerate(filenames):
            f[fn] = sparse.load_npz(namedir + filestub + ".h5" + fn + ".npz")

        canvas = np.zeros([nevents, 900, 1115], dtype=bool)

        for iimg in range(nevents):
            for ilab, irow in quartile_filter.iterrows():
                img = f[ilab][iimg].todense()
                fimg = maximum_filter(img > irow.qval, irow["filter"])
                if irow.t: fimg = fimg.T
                y = int(irow.y)
                x = int(irow.x)
                if irow.x == 850:
                    y += int(186 * irow.idq)
                else:
                    y += int(372 * irow.idq)
                canvas[iimg, x:x + fimg.shape[0], y:y + fimg.shape[1]] = fimg

        return canvas, labels

    # Output directory, change if needed
    outfile = "./opuout/" + f"{collision_type}_AllEvtSparse_{n_comp}_random"
    print("Output files:", outfile)

    with random_mapping.opu:
        for i, filestubn in enumerate(tqdm(range(260))):
            filestub = f"{filestubn}"
            arr, labels = file2arr(filestub)
            if i == 0:
                OPUoutput = random_mapping.fit_transform(arr)
            else:
                OPUoutput = random_mapping.transform(arr)
            np.savez_compressed(outfile + filestub,
                                OPU=OPUoutput,
                                labels=labels)